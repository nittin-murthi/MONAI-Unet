{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import ArrayDataset, create_test_image_2d, decollate_batch, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    RandRotate90,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def main(tempdir):\n",
    "    \n",
    "    # Define paths to image and mask directories\n",
    "    image_dir = os.path.join(tempdir, \"image\")\n",
    "    mask_dir = os.path.join(tempdir, \"mask\")\n",
    "\n",
    "    # Read image and mask file paths\n",
    "    images = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "    masks = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "\n",
    "    if len(images) != len(masks):\n",
    "        raise ValueError(\"The number of images and masks must be the same.\")\n",
    "    \n",
    "    # define transforms for image and segmentation\n",
    "    train_imtrans = Compose(\n",
    "        [\n",
    "            LoadImage(image_only=True, ensure_channel_first=True),\n",
    "            ScaleIntensity(),\n",
    "            RandSpatialCrop((128, 128), random_size=False),\n",
    "            RandRotate90(prob=0.5, spatial_axes=(0, 1)),\n",
    "        ]\n",
    "    )\n",
    "    train_masktrans = Compose(\n",
    "        [\n",
    "            LoadImage(image_only=True, ensure_channel_first=True),\n",
    "            ScaleIntensity(),\n",
    "            RandSpatialCrop((128, 128), random_size=False),\n",
    "            RandRotate90(prob=0.5, spatial_axes=(0, 1)),\n",
    "        ]\n",
    "    )\n",
    "    val_imtrans = Compose([LoadImage(image_only=True, ensure_channel_first=True), ScaleIntensity()])\n",
    "    val_masktrans = Compose([LoadImage(image_only=True, ensure_channel_first=True), ScaleIntensity()])\n",
    "\n",
    "    # define array dataset, data loader\n",
    "    check_ds = ArrayDataset(images, train_imtrans, masks, train_masktrans)\n",
    "    check_loader = DataLoader(check_ds, batch_size=10, num_workers=2, pin_memory=True)\n",
    "    im, msk = monai.utils.misc.first(check_loader)\n",
    "    print(im.shape, msk.shape)\n",
    "    \n",
    "    # create a training data loader\n",
    "    train_ds = ArrayDataset(images[:20], train_imtrans, masks[:20], train_masktrans)\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=8, pin_memory=torch.backends.mps.is_available())\n",
    "    # create a validation data loader\n",
    "    val_ds = ArrayDataset(images[-20:], val_imtrans, masks[-20:], val_masktrans)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    \n",
    "    # create UNet, DiceLoss and Adam optimizer\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = monai.networks.nets.UNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=3,\n",
    "        out_channels=1,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "    ).to(device)\n",
    "    loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "    # start a typical PyTorch training\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(10):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{10}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_images = None\n",
    "                val_labels = None\n",
    "                val_outputs = None\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                    roi_size = (128, 128)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n",
    "                    val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                    # compute metric for current iteration\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                # reset the status for next validation round\n",
    "                dice_metric.reset()\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), \"best_metric_model_segmentation2d_array.pth\")\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    \"current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}\".format(\n",
    "                        epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                    )\n",
    "                )\n",
    "                writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "                # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n",
    "                plot_2d_or_3d_image(val_images, epoch + 1, writer, index=0, tag=\"image\")\n",
    "                plot_2d_or_3d_image(val_labels, epoch + 1, writer, index=0, tag=\"label\")\n",
    "                plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=\"output\")\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 128, 128]) torch.Size([10, 1, 128, 128])\n",
      "----------\n",
      "epoch 1/10\n",
      "1/5, train_loss: 0.8579\n",
      "2/5, train_loss: 0.6299\n",
      "3/5, train_loss: 0.9263\n",
      "4/5, train_loss: 0.8615\n",
      "5/5, train_loss: 0.8319\n",
      "epoch 1 average loss: 0.8215\n",
      "----------\n",
      "epoch 2/10\n",
      "1/5, train_loss: 0.8687\n",
      "2/5, train_loss: 0.7825\n",
      "3/5, train_loss: 0.8348\n",
      "4/5, train_loss: 0.8459\n",
      "5/5, train_loss: 0.9885\n",
      "epoch 2 average loss: 0.8641\n",
      "saved new best metric model\n",
      "current epoch: 2 current mean dice: 0.2361 best mean dice: 0.2361 at epoch 2\n",
      "----------\n",
      "epoch 3/10\n",
      "1/5, train_loss: 0.8936\n",
      "2/5, train_loss: 0.8390\n",
      "3/5, train_loss: 0.6793\n",
      "4/5, train_loss: 0.4085\n",
      "5/5, train_loss: 0.8012\n",
      "epoch 3 average loss: 0.7243\n",
      "----------\n",
      "epoch 4/10\n",
      "1/5, train_loss: 0.7385\n",
      "2/5, train_loss: 0.8314\n",
      "3/5, train_loss: 0.6632\n",
      "4/5, train_loss: 0.8089\n",
      "5/5, train_loss: 0.8965\n",
      "epoch 4 average loss: 0.7877\n",
      "saved new best metric model\n",
      "current epoch: 4 current mean dice: 0.2508 best mean dice: 0.2508 at epoch 4\n",
      "----------\n",
      "epoch 5/10\n",
      "1/5, train_loss: 0.9457\n",
      "2/5, train_loss: 0.7314\n",
      "3/5, train_loss: 0.7276\n",
      "4/5, train_loss: 0.7522\n",
      "5/5, train_loss: 0.8328\n",
      "epoch 5 average loss: 0.7980\n",
      "----------\n",
      "epoch 6/10\n",
      "1/5, train_loss: 0.9812\n",
      "2/5, train_loss: 0.6719\n",
      "3/5, train_loss: 0.7618\n",
      "4/5, train_loss: 0.8515\n",
      "5/5, train_loss: 0.8160\n",
      "epoch 6 average loss: 0.8165\n",
      "saved new best metric model\n",
      "current epoch: 6 current mean dice: 0.2584 best mean dice: 0.2584 at epoch 6\n",
      "----------\n",
      "epoch 7/10\n",
      "1/5, train_loss: 0.7366\n",
      "2/5, train_loss: 0.6683\n",
      "3/5, train_loss: 0.8422\n",
      "4/5, train_loss: 0.9114\n",
      "5/5, train_loss: 0.7633\n",
      "epoch 7 average loss: 0.7843\n",
      "----------\n",
      "epoch 8/10\n",
      "1/5, train_loss: 0.6884\n",
      "2/5, train_loss: 0.7508\n",
      "3/5, train_loss: 0.9112\n",
      "4/5, train_loss: 0.6891\n",
      "5/5, train_loss: 1.0000\n",
      "epoch 8 average loss: 0.8079\n",
      "saved new best metric model\n",
      "current epoch: 8 current mean dice: 0.2617 best mean dice: 0.2617 at epoch 8\n",
      "----------\n",
      "epoch 9/10\n",
      "1/5, train_loss: 0.9544\n",
      "2/5, train_loss: 1.0000\n",
      "3/5, train_loss: 0.8250\n",
      "4/5, train_loss: 0.6255\n",
      "5/5, train_loss: 0.8563\n",
      "epoch 9 average loss: 0.8523\n",
      "----------\n",
      "epoch 10/10\n",
      "1/5, train_loss: 0.6500\n",
      "2/5, train_loss: 0.9334\n",
      "3/5, train_loss: 0.9996\n",
      "4/5, train_loss: 0.8133\n",
      "5/5, train_loss: 0.8408\n",
      "epoch 10 average loss: 0.8474\n",
      "current epoch: 10 current mean dice: 0.2367 best mean dice: 0.2617 at epoch 8\n",
      "train completed, best_metric: 0.2617 at epoch: 8\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\"/Users/nittin_murthi/Documents/VS_Code/MONAI-UNET/segmentation_data/malignant/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
